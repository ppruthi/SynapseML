from pyspark.sql.functions import udf, col
from pyspark.sql.types import StructType,StructField, DoubleType
from pyspark.sql.functions import lit
from pyspark.ml import PipelineModel
from pyspark.sql.functions import col
import os
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# Configure more resiliant requests to stop flakiness
retry_strategy = Retry(
    total=3,
    status_forcelist=[429, 500, 502, 503, 504],
    method_whitelist=["HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE"]
)
adapter = HTTPAdapter(max_retries=retry_strategy)
http = requests.Session()
http.mount("https://", adapter)
http.mount("http://", adapter)

if os.environ.get("AZURE_SERVICE", None) == "Microsoft.ProjectArcadia":
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()
    from notebookutils.mssparkutils.credentials import getSecret
    os.environ['AZURE_MAPS_KEY'] = getSecret(
        "mmlspark-keys", "azuremaps-api-key")

from synapse.ml.cognitive import *
from synapse.ml.geospatial import *

# An Azure Maps account key
azureMapsKey = os.environ["AZURE_MAPS_KEY"]

from synapse.ml.stages import FixedMiniBatchTransformer, FlattenBatch

df = spark.createDataFrame([
    ("One, Microsoft Way, Redmond",),
    ("400 Broad St, Seattle",),
    ("350 5th Ave, New York",),
    ("Pike Pl, Seattle",),
    ("Champ de Mars, 5 Avenue Anatole France, 75007 Paris",)
], ["address",])


def extract_location_fields(df):
    # Use this function to select only lat/lon columns into the dataframe
    return df.select(col("*"),
        col("output.response.results").getItem(0).getField("position").getField("lat").alias("Latitude"),
        col("output.response.results").getItem(0).getField("position").getField("lon").alias("Longitude")
    ).drop("output")

# Run the Azure Maps geocoder to enhance the data with location data
geocoder = (AddressGeocoder()
    .setSubscriptionKey(azureMapsKey)
    .setAddressCol("address")
    .setOutputCol("output"))

# Show the results of your text query in a table format
display(extract_location_fields(geocoder.transform(FixedMiniBatchTransformer().setBatchSize(10).transform(df))))

# Create a dataframe that's tied to it's column names
df = spark.createDataFrame(((
    (48.858561, 2.294911),
    (47.639765, -122.127896),
    (47.621028, -122.348170),
    (47.734012, -122.102737)
  )), StructType([StructField("lat", DoubleType()), StructField("lon", DoubleType())]))

# Run the Azure Maps geocoder to enhance the data with location data
rev_geocoder = (ReverseAddressGeocoder()
    .setSubscriptionKey(azureMapsKey)
    .setLatitudeCol("lat")
    .setLongitudeCol("lon")
    .setOutputCol("output"))

# Show the results of your text query in a table format

display(rev_geocoder.transform(FixedMiniBatchTransformer().setBatchSize(10).transform(df)).select(col("*"),
      col("output.response.addresses").getItem(0).getField("address").getField("freeformAddress").alias("In Polygon"),
      col("output.response.addresses").getItem(0).getField("address").getField("country").alias("Intersecting Polygons")
    ).drop("output"))


import time
import json

# Choose a geography, you want your data to reside in.
# Allowed values 
# us => North American datacenters
# eu -> European datacenters
url_geo_prefix = 'us' 

# Upload a geojson with polygons in them
r= http.post(f'https://{url_geo_prefix}.atlas.microsoft.com/mapData/upload?api-version=1.0&dataFormat=geojson&subscription-key={azureMapsKey}',
    json= { 
        "type": "FeatureCollection", 
        "features": [
            {
                "type": "Feature",
                "properties": { "geometryId": "test_geometry" },
                "geometry": {
                    "type": "Polygon",
                    "coordinates":[
                        [
                            [
                                -122.14290618896484,
                                47.67856488312544
                            ],
                            [
                                -122.03956604003906,
                                47.67856488312544
                            ],
                            [
                                -122.03956604003906,
                                47.7483271435476
                            ],
                            [
                                -122.14290618896484,
                                47.7483271435476
                            ],
                            [
                                -122.14290618896484,
                                47.67856488312544
                            ]
                        ]
                    ]
                } 
            } 
        ] 
    })

long_running_operation = r.headers.get('location')
time.sleep(30) # Sometimes this may take upto 30 seconds
print(f"Status Code: {r.status_code}, Long Running Operation: {long_running_operation}")
# This Operation completes in approximately 5 ~ 15 seconds 
user_data_id_resource_url = json.loads(http.get(f'{long_running_operation}&subscription-key={azureMapsKey}').content)['resourceLocation']
user_data_id = json.loads(http.get(f'{user_data_id_resource_url}&subscription-key={azureMapsKey}').content)['udid']

# Create a dataframe that's tied to it's column names
df = spark.createDataFrame(((
    (48.858561, 2.294911),
    (47.639765, -122.127896),
    (47.621028, -122.348170),
    (47.734012, -122.102737)
  )), StructType([StructField("lat", DoubleType()), StructField("lon", DoubleType())]))

# Run the Azure Maps geocoder to enhance the data with location data
check_point_in_polygon = (CheckPointInPolygon()
    .setSubscriptionKey(azureMapsKey)
    .setGeography(url_geo_prefix)
    .setUserDataIdentifier(user_data_id)
    .setLatitudeCol("lat")
    .setLongitudeCol("lon")
    .setOutputCol("output"))

# Show the results of your text query in a table format
display(check_point_in_polygon.transform(df).select(col("*"),
        col("output.result.pointInPolygons").alias("In Polygon"),
        col("output.result.intersectingGeometries").alias("Intersecting Polygons")
    ).drop("output"))

res = http.delete(f"https://{url_geo_prefix}.atlas.microsoft.com/mapData/{user_data_id}?api-version=1.0&subscription-key={azureMapsKey}")
